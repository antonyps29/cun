# -*- coding: utf-8 -*-
"""modelo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10YTTILefKPIA6sjjFMiafJDgTns3Dkrn
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.neighbors import KNeighborsRegressor
from skforecast.model_selection import grid_search_forecaster
from skforecast.model_selection import backtesting_forecaster
from skforecast.ForecasterAutoreg import ForecasterAutoreg
import time
time.sleep(5)

#Cargar data
data_modelo=pd.read_csv('data_modelo.csv')
data_final=pd.read_csv('data_final.csv')

# Number of predictions
steps = 60
# Training and testing sets to search for optimal parameters
data_train = data_modelo[:-steps]
data_test  = data_modelo[-steps:]

# Hiperparametros grid search usaremos random forest como primer modelo
forecaster = ForecasterAutoreg(regressor = RandomForestRegressor(random_state=123),
                               lags=7)

# Lags para usar como predictores
lags_grid = [7, 14, 30]

# Definimos los hiperparametroas a usar en los modelos
param_grid = {'n_estimators': [20,50,100],
              'max_depth': [5,10,20]}

results_grid = grid_search_forecaster(forecaster         = forecaster,
                                      y                  = data_train['y'],
                                      param_grid         = param_grid,
                                      lags_grid          = lags_grid,
                                      steps              = steps,
                                      refit              = True,
                                      metric             = 'mean_squared_error',
                                      initial_train_size =  max(int(len(data_train) * 0.5), max(lags_grid) + 1),
                                      fixed_train_size   = False,
                                      return_best        = True,
                                      verbose            = False)

# Número total de predicciones
steps = 90

# Última fecha en el conjunto de entrenamiento
last_train_date = data_train['ds'].max()
last_train_date = pd.to_datetime(last_train_date)

# Generar fechas a partir del día siguiente al último día de train
prediction_dates = pd.date_range(start=last_train_date + timedelta(days=1), periods=steps, freq='D')

# Realizar las predicciones con el modelo ya entrenado
sales_predictions = forecaster.predict(steps=steps)

# Crear el DataFrame con las fechas y las predicciones
data_predictions = pd.DataFrame({
    'date': prediction_dates,
    'sales_prediction': sales_predictions
})

data_predictions['date'] = pd.to_datetime(data_predictions['date'])
data_final['date'] = pd.to_datetime(data_final['date'])
data_final = pd.merge(data_final, data_predictions, on='date', how='left')

#normalizamos
fechas_con_pred = data_final[~data_final['sales_prediction'].isna()]['date']

# Contar ocurrencias de cada fecha y guardar como diccionario
conteo_fechas_pred = fechas_con_pred.value_counts().to_dict()

# Aplicar división solo si la predicción no es nula
data_final['sales_prediction'] = data_final.apply(
    lambda row: row['sales_prediction'] / conteo_fechas_pred[row['date']]
    if pd.notnull(row['sales_prediction']) else np.nan,
    axis=1
)


data_final.to_csv('data_predicha.csv', index=False)

